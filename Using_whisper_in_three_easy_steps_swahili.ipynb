{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kaarimu/whisper_swahili/blob/main/Using_whisper_in_three_easy_steps_swahili.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size=64px>Try Whisper in Three Easy Steps</font><a href=\"https://deepgram.com/openai-whisper\"><img src=\"https://drive.google.com/uc?id=1SPAig9IJ_0cBG-7Vzf7fP4vEDON0HeIO\"\n",
        "height=\"64\" align=\"right\"></a>\n",
        "\n",
        "<font size=2px>By Ross O'Connell</font>\n",
        "\n",
        "Whisper is an exciting new model for automatic speech recognition (ASR) developed by OpenAI. There are a few potential pitfalls to installing it on a local machine, so speech recognition experts at [Deepgram](https://deepgram-blog.ghost.io/ghost/#/editor/post/63374e260072bc003d64fd6a) have put together this Colab notebook. Our goal is to make it super easy for everybody to see what Whisper can do!\n",
        "\n",
        "**We chose some fun audio to transcribe â€“ can you identify it from Whisper's transcription?**"
      ],
      "metadata": {
        "id": "9qzwD9ts4_kc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the first line we install Whisper!"
      ],
      "metadata": {
        "id": "V1QHcVQz4Gu7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOvKw2K3kWqK"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we pull down some audio to transcribe."
      ],
      "metadata": {
        "id": "FatSKi3YAQCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yt-dlp\n",
        "!yt-dlp https://www.youtube.com/watch?v=dQw4w9WgXcQ --format m4a -o \"%(id)s.%(ext)s\""
      ],
      "metadata": {
        "id": "cYaGfY1J2VRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we run Whisper! It may take a little time to get started, but soon the transcription should start to appear."
      ],
      "metadata": {
        "id": "yZ2-aeeBAefF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!whisper \"/content/dQw4w9WgXcQ.m4a\" --model small --language English"
      ],
      "metadata": {
        "id": "uU1Bxv1y6zAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking Whisper's Work\n",
        "\n",
        "Whisper hasn't just produced text, it's given us time intervals where it believes that text occurred. In this section we'll read in Whisper's transcript, split up the audio according to Whisper's timestamps, and then print Whisper's text and play the corresponding audio. How well do they match?"
      ],
      "metadata": {
        "id": "qOtm7PV0WwR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import IPython.display as ipd\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "RITBdx3FXC5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whisper's output is saved in `.vtt` format; we'll install `webvtt-py`, a package that can read that format."
      ],
      "metadata": {
        "id": "SMy80hPTgtZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install webvtt-py"
      ],
      "metadata": {
        "id": "aKQ4gmnnBj85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import webvtt"
      ],
      "metadata": {
        "id": "C_C03ZyYWni9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`librosa` is a library for reading and manipulating audio files."
      ],
      "metadata": {
        "id": "T515Pxovg2Ug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa"
      ],
      "metadata": {
        "id": "5XJDcmz1YJ4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have two custom functions here, one to convert H:M:S timestamps into seconds, and another to trim out a chunk of audio corresponding to a particular `start` and `end` time."
      ],
      "metadata": {
        "id": "F12-3a4phEMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_hms(s):\n",
        "  h,m,sec = [float(x) for x in s.split(':')]\n",
        "  return 3600*h + 60*m + sec"
      ],
      "metadata": {
        "id": "qopgBohKZ8vI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trim_audio(row,audio,sample_rate):\n",
        "  t = np.arange(len(audio))\n",
        "  t = t/sample_rate\n",
        "  f = np.where( (t>=row.start_s) & (t<=row.end_s) )\n",
        "  return audio[f]"
      ],
      "metadata": {
        "id": "bXBSOwKHbKfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As promised, we use `webvtt` to read in the transcript and `librosa` to read in the audio."
      ],
      "metadata": {
        "id": "Zis6Xr1LhQeO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transcript = webvtt.read('/content/dQw4w9WgXcQ.m4a.vtt')\n",
        "audio,sample_rate = librosa.load('/content/dQw4w9WgXcQ.m4a')"
      ],
      "metadata": {
        "id": "P3p1-1svW7R8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ovahAWYY4apr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For convenience we're going to set up a Pandas dataframe to store the various quantities we want to track. Each row will correspond to one segment of the Whisper transcript."
      ],
      "metadata": {
        "id": "pgmVVTWbhYAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(columns=['start','end','text'])\n",
        "\n",
        "df['start'] = [x.start for x in transcript]\n",
        "df['end'] = [x.end for x in transcript]\n",
        "df['text'] = [x.text for x in transcript]\n",
        "df['start_s'] = df['start'].apply(simple_hms)\n",
        "df['end_s'] = df['end'].apply(simple_hms)\n",
        "df['audio'] = df.apply(trim_audio,axis=1,args=(audio,sample_rate))\n",
        "df.head()"
      ],
      "metadata": {
        "id": "fUYKIyXAXYTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we'll grab a random segment of the Whisper transcript, print out the text, and play the audio. If there's a particular segment you want to look at you can **comment out** the `segment = df.sample...` line, **uncomment** the `segment = df.loc...` line, and enter the number of the segment you want to see!"
      ],
      "metadata": {
        "id": "8feqzXBXhhE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "segment = df.sample(n=1).iloc[0]\n",
        "# segment = df.loc[16]\n",
        "print(segment.text)\n",
        "ipd.Audio(segment.audio,rate=sample_rate)"
      ],
      "metadata": {
        "id": "PLgwOtaiXc8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Looking at Whisper's Word Error Rate (WER)\n",
        "\n",
        "A simple way to quantify Whisper's performance is to look at its [Word Error Rate](https://blog.deepgram.com/what-is-word-error-rate/) (WER). In this section we're going to load a new audio source, an anonymous reading of a newspaper article, as well as the text of the article. We'll compare Whisper's transcript to the true text and look at the WER!"
      ],
      "metadata": {
        "id": "kbXuqzqajZ1W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our example for this section will be \"Nikola Tesla Sees a Wireless Vision\", an article from the New York Times from 1915.  We'll download two files:\n",
        "\n",
        "* The audio file, a volunteer reading the article for [LibriVox](https://librivox.org/short-nonfiction-collection-vol-025-by-various/)\n",
        "* A text file with the original text of the article"
      ],
      "metadata": {
        "id": "9LuBUEUo2rgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://static.deepgram.com/examples/snf025_nikolateslawirelessvision_anonymous_gu.mp3\n",
        "!wget https://static.deepgram.com/examples/Nikola_Tesla_Sees_a_Wireless_Vision.txt"
      ],
      "metadata": {
        "id": "rVO4lQmgl70o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've got the audio, we'll have Whisper transcribe it!"
      ],
      "metadata": {
        "id": "o6bmLpeV20o8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!whisper \"/content/snf025_nikolateslawirelessvision_anonymous_gu.mp3\" --model small --language English"
      ],
      "metadata": {
        "id": "as13EvR_kGrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we'll read in the text file that Whisper has generated. There are a few lines that describe the recording and are not part of the original text, so we'll strip those out."
      ],
      "metadata": {
        "id": "hpmfmK_H24l4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/snf025_nikolateslawirelessvision_anonymous_gu.mp3.txt','r') as f:\n",
        "  whisper_lines = [l.strip() for l in f]\n",
        "\n",
        "#Stripping out the boilerplate at the beginning and end of the file.\n",
        "whisper_lines = whisper_lines[4:]\n",
        "whisper_lines = whisper_lines[:-2]"
      ],
      "metadata": {
        "id": "UaUrkqKUxpfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We downloaded the true text of the article earlier, we just need to read it in:"
      ],
      "metadata": {
        "id": "8sMtCIRL3GEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/Nikola_Tesla_Sees_a_Wireless_Vision.txt','r') as f:\n",
        "  true_lines = [l.strip() for l in f]"
      ],
      "metadata": {
        "id": "xmNF6uy4ZqZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's time for a little bit of **text cleaning**. Although Whisper seems to be pretty good at capitalization, there is some unusual use of capitalization in the original text that Whisper couldn't know about. We don't want to penalize Whisper for that, so we'll convert all text to lowercase. We're also going to remove quotation marks -- again, Whisper seems pretty good at getting these in the right place, but we'd like to focus on words for now.\n",
        "\n",
        "There's more text cleaning we could do -- for example, we could be much more careful with now **numbers** and **numerals** are handled. For this demonstration, though, we're going to keep things simple!"
      ],
      "metadata": {
        "id": "x6y7Tent3Qtp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "whisper_text = \" \".join(whisper_lines)\n",
        "whisper_text = whisper_text.replace('\"','')\n",
        "whisper_text = whisper_text.lower()\n",
        "\n",
        "true_text = \" \".join(true_lines)\n",
        "true_text = true_text.replace('\"','')\n",
        "true_text = true_text.lower()"
      ],
      "metadata": {
        "id": "vN4YakbRzOcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With all that done, we're ready to compute the WER. How did Whisper do?"
      ],
      "metadata": {
        "id": "ZETIj3ff32UY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jiwer"
      ],
      "metadata": {
        "id": "VprT5sSTZS3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from jiwer import wer\n",
        "\n",
        "wer(true_text,whisper_text)"
      ],
      "metadata": {
        "id": "RjvXbLvq0qju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that this is how Whisper performs in optimal circumstances, with a single, clear speaker, in other circumstances its performance may be very different! Deepgram researchers will take a look at that in an [upcoming blog post](https://blog.deepgram.com/)."
      ],
      "metadata": {
        "id": "ioy7EjKS6mWf"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}